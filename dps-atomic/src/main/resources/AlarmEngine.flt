package ${packagePath}

import scala.collection.mutable.Map

import org.apache.spark.SparkConf
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.Dataset
import org.apache.spark.sql.Row
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.SparkSession

import dps.atomic.define.AtomOperationDefine
import dps.atomic.define.AtomOperationParamDefine

class ${className}(override val sparkSession: SparkSession, override val sparkConf:SparkConf,override val inputVariableKey: String, override val outputVariableKey: String, override val variables: Map[String, Any]) extends AbstractAction(sparkSession, sparkConf,inputVariableKey, outputVariableKey, variables) with Serializable {

   def doIt(params: Map[String, String]): Any = {

    val ruleExtends: RDD[Map[String, Any]] = ruleSplicing(params)
    
    alarmOriginalHandle(ruleExtends, params)

    alarmActiveHandle(ruleExtends, params)

  }

  /**
   * 活动告警条件过滤并存储
   */
  private def alarmActiveHandle(rules: RDD[Map[String, Any]], params: Map[String, String]) = {
    rules.collect().foreach(m => {
      val alarmSql = s"""(
                         ${activeSql}
                          ) as tmpAlarmOriginal""".stripMargin
      val alarmOriginal: Dataset[Row] = this.jdbcQuery(params, alarmSql)

      val activeSql = s"""(
                          select uuid() as id,
                                 tmpAlarmOriginal.alarm_content as alarm_content,
                                 tmpAlarmOriginal.alarm_level as alarm_level,
                                 tmpRuleView.alarm_rule_name as alarm_title,
                                 tmpAlarmOriginal.identification_field as identification_field,
                                 tmpAlarmOriginal.occur_time as occur_time,
                                 tmpRuleView.alarm_rule_id as alarm_rule_id,
                                 now() as create_time
                            from tmpAlarmOriginal inner join tmpRuleView
                            where tmpAlarmOriginal.alarm_rule_id = tmpRuleView.alarm_rule_id
                          ) as tmpAlarmActive""".stripMargin

      val dataset = sparkSession.sqlContext.sql(activeSql);
      val dscount = dataset.count()
      if (dscount > 0) {
        this.store2db(dataset, params, "b_alarm", SaveMode.Append)
      } else {
        println("+------------------------------+")
        println("无数据,跳过存储操作")
        println("+------------------------------+")
      }
    })
  }

  /**
   * 过滤原始告警并存储
   *
   * @param rules
   * @param params
   */
  private def alarmOriginalHandle(rules: RDD[Map[String, Any]], params: Map[String, String]) = {
    rules.collect().foreach(m => {
      val alarmSql = s"""${originalSql}"""
      val dataset = sparkSession.sqlContext.sql(alarmSql);
      val dscount = dataset.count()
      if (dscount > 0) {
        this.store2db(dataset, params, "b_alarm_original", SaveMode.Append)
      } else {
        println("+------------------------------+")
        println("无数据,跳过存储操作")
        println("+------------------------------+")
      }
    })
  }

  /**
   * 数据存储到关系型数据库
   *
   * @param dataset
   * @param params
   * @param dbtable
   * @param savemode
   */
  private def store2db(dataset: Dataset[Row], params: Map[String, String], dbtable: String, savemode: SaveMode) = {
    dataset.write.format("jdbc")
      .option("driver", params.get("driver").get)
      .option("url", params.get("url").get)
      .option("dbtable", dbtable)
      .option("user", params.get("user").get)
      .option("password", params.get("password").get)
      .mode(savemode).save();
  }

  /**
   * JDBC查询返回Dataset[Row]
   *
   * @param params
   * @param sql
   * @return
   */
  private def jdbcQuery(params: Map[String, String], sql: String): Dataset[Row] = {
    sparkSession.sqlContext.read.format("jdbc")
      .option("url", params.get("url").get)
      .option("driver", params.get("driver").get)
      .option("dbtable", sql)
      .option("user", params.get("user").get)
      .option("password", params.get("password").get)
      .load();
  }

}