package ${packagePath}

import dps.atomic.impl.AbstractAction
import dps.atomic.define.{AtomOperationDefine, AtomOperationParamDefine}
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{Dataset, Row, SaveMode, SparkSession}

import scala.collection.mutable.Map
import org.apache.spark.SparkConf
import java.util.ArrayList

class ${className}(override val sparkSession: SparkSession, override val sparkConf:SparkConf,override val inputVariableKey: String, override val outputVariableKey: String, override val variables: Map[String, Any]) extends AbstractAction(sparkSession, sparkConf,inputVariableKey, outputVariableKey, variables) with Serializable {

   def doIt(params: Map[String, String]): Any = {
    var alarmRules = variables.get("alarmRules").asInstanceOf[Dataset[Row]]
    var baseRuleAlarms = variables.get("baseRuleAlarms").asInstanceOf[Dataset[Row]]
    alarmRules.rdd.map(row=>{
      (row.getAs("id").asInstanceOf[String],row.getAs("occur_count").asInstanceOf[Integer],row.getAs("open_upgrade").asInstanceOf[Integer])
    })
  }

}