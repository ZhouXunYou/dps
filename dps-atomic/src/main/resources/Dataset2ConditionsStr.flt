package ${packagePath}

import dps.atomic.impl.AbstractAction
import dps.atomic.define.{AtomOperationDefine, AtomOperationParamDefine}
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{Dataset, Row, SparkSession}

import scala.collection.mutable.Map
import org.apache.spark.SparkConf

class ${className}(override val sparkSession: SparkSession, override val sparkConf:SparkConf,override val inputVariableKey: String, override val outputVariableKey: String, override val variables: Map[String, Any]) extends AbstractAction(sparkSession, sparkConf,inputVariableKey, outputVariableKey, variables) with Serializable {
   def doIt(params: Map[String, String]): Any = {
    val dataset = this.pendingData.asInstanceOf[Dataset[Row]];

    val arr = dataset.collect()
    val map = Map[String, String]()
    arr.foreach(row => {
    	println(row.getString(0))
      val key = row.getString(5)
      val condition = map.get(key)
      var str = row.getString(3).concat(row.getString(7)).concat(row.getString(2))
      if (!condition.isEmpty) {
        str = condition.get.concat(" and ").concat(str);
      }
      println(str)
      map.put(row.getString(0), str)
    })

    println(map)
    this.variables.put(outputVariableKey, map);
  }

    /**
    * 将数据转换成where条件
    *
    * @param array
    * @return
    */
    private def dataHandle(array: Array[Row]): Map[String, String] = {
        ${dataHandleCode}
    }

}