package ${packagePath}

import dps.atomic.define.{AtomOperationDefine, AtomOperationParamDefine}
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession

import scala.collection.mutable.Map

class ${className}(override val sparkSession: SparkSession, override val inputVariableKey: String, override val outputVariableKey: String, override val variables: Map[String, Any]) extends dps.atomic.impl.AbstractAction(sparkSession,inputVariableKey, outputVariableKey, variables) with Serializable {
  
  def doIt(params: Map[String, String]): Any = {
    val kafkTuple = this.pendingData.asInstanceOf[RDD[Tuple3[String, Int, String]]]
    val groupTopic = kafkTuple.groupBy(tuple => tuple._1).collect()
    groupTopic.foreach(topic => {
      val topicName = topic._1
      val topicRDD = sparkSession.sparkContext.parallelize(topic._2.toSeq)
      val valueRDD = topicRDD.map(tuple=>{
        tuple._3
      })
      variables.put(outputVariableKey + "_" + topicName, valueRDD)
    })
  }
}