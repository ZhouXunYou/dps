package ${packagePath}
import scala.collection.mutable.Map
import org.apache.spark.SparkContext
import org.apache.spark.sql.SQLContext
import dps.atomic.define.AtomOperationDefine
import dps.atomic.define.AtomOperationParamDefine
import dps.atomic.define.AtomOperationDefine
import dps.atomic.impl.AbstractAction

class ${className}(override val sparkContext:SparkContext, override val inputVariableKey: String, override val outputVariableKey: String, override val variables: Map[String, Any]) extends AbstractAction(sparkContext,inputVariableKey, outputVariableKey, variables) with Serializable {
  
  def doIt(params: Map[String, String]): Any = {
    val kafkTuple = this.pendingData.asInstanceOf[RDD[Tuple3[String,Int,String]]]
    kafkTuple.groupBy(tuple=>{
      tuple._1
    }).map(topicLines=>{
      val topicName = topicLines._1
      val topicRDD = sparkContext.parallelize(topicLines._2.toSeq)
      val stringRdd = topicRDD.map(topicTuple=>{
        topicTuple._3
      })
      variables.put(outputVariableKey+"_"+topicName, stringRdd)
    })
  }
}