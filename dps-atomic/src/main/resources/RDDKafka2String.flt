package ${packagePath}

import dps.atomic.define.{AtomOperationDefine, AtomOperationParamDefine}
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession

import scala.collection.mutable.Map

class ${className}(override val sparkSession: SparkSession, override val inputVariableKey: String, override val outputVariableKey: String, override val variables: Map[String, Any]) extends AbstractAction(sparkSession,inputVariableKey, outputVariableKey, variables) with Serializable {
  
  def doIt(params: Map[String, String]): Any = {
      val kafkTuple = this.pendingData.asInstanceOf[RDD[Tuple3[String,Int,String]]]
      val groupTopic = kafkTuple.groupBy(tuple=>tuple._1).collect()
      groupTopic.foreach(topic=>{
        val topicName = topic._1
        val topicRDD = sparkSession.sparkContext.parallelize(topic._2.toSeq)
        variables.put(outputVariableKey+"_"+topicName, topicRDD)
      })
  }
}